/opt/anaconda3/lib/python3.11/site-packages/torch/cuda/amp/grad_scaler.py:126: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.
  warnings.warn(
Starting Training Loop...
Traceback (most recent call last):
  File "/Users/akommula/code/research/char_level_model_transformer/baseline/train.py", line 169, in <module>
    losses = estimate_loss()
             ^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/akommula/code/research/char_level_model_transformer/baseline/train.py", line 124, in estimate_loss
    X, Y = batchify(data)
           ^^^^^^^^^^^^^^
  File "/Users/akommula/code/research/char_level_model_transformer/baseline/train.py", line 81, in batchify
    ix = torch.randint(len(data) - block_size, (batch_size,))
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: random_ expects 'from' to be less than 'to', but got from=0 >= to=-511
